---
title: "CUNY SPS DATA 622 - Machine Learning and Big Data"
subtitle: 'Spring 2021 - Group 3 - Homework 3'
author: "Maryluz Cruz, Samantha Deokinanan, Amber Ferger, Tony Mei, and Charlie Rosemond"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document
urlcolor: purple
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center")
```

## R Packages

The statistical tool that will be used to facilitate the modeling of the data was `R`. The main packages used for data wrangling, visualization, and graphics were listed below.

```{r libraries, echo=TRUE}
# Required R packages
library(palmerpenguins)
library(tidyverse)
library(kableExtra)
library(summarytools)
library(GGally)
library(caret)
library(mice)
library(dummies)
library(Boruta)
library(pROC)
library(rsample)
library(gbm)
```

## Overview {.tabset .tabset-fade .tabset.-pills}


***
<center> **PROJECT SECTIONS** </center>
***

### Palmer Penguins 

#### Data Exploration 

The `palmerpenguins` data contains size measurements collected from 2007 - 2009 for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. For more information about this data collection, refer to  [palmerpenguins website.](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)

*Penguins Data Column Definition*

Variable | Description
----|------
species | penguin species (Ad√©lie, Chinstrap, and Gentoo)
island | island in Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)
bill_length_mm | bill length (millimeters)
bill_depth_mm | bill depth (millimeters)
flipper_length_mm | flipper length (millimeters)
body_mass_g | body mass (grams)
sex | penguin sex (female, male)
year | year data was collected

```{r loadpeng}
# Load dataset
penguins = penguins

# Number of observations
ntrobs = dim(penguins)[[1]]

# Converting Year to factor
penguins$year = as.factor(penguins$year)
```

From the previous data exploration, it found that the response variable, `species` denotes one of three penguin species, and a majority of the penguins are Adelie (n = 153), followed by Gentoo (n = 124) and Chinstrap (n = 68). The distribution between gender is nearly equally divided among the species but not for their island habitat. 

```{r pengdist, fig.width=8}
reorder <- function(x){
  factor(x, levels = names(sort(table(x), decreasing = TRUE)))
}

ggplot(drop_na(penguins), aes(x = reorder(species), fill = species)) + 
  geom_bar() +
  geom_text(stat = "count", aes(label =..count..), vjust=-0.5, size = 3) +
  facet_wrap(~sex) +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +
  theme(legend.position = "none")+
  labs(title = "Distibution of Species by Gender", y = "Frequency", x = "Species")
  
 ggplot(drop_na(penguins), aes(x = reorder(species), fill = species)) + 
  geom_bar() +
  geom_text(stat = "count", aes(label =..count..), vjust=-0.5, size = 3) +
  facet_wrap(~island) +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +
  theme(legend.position = "none")+
  labs(title = "Distibution of Species by Island Habitat", y = "Frequency", x = "Species")
```

There were `r ntrobs` observations of 4 numeric predictor variables and 2-factor predictor variables, namely `island`, and `sex`.  There is also a `year` variable that is ignored in this analysis. The data set did not have complete cases, and there is a presence of bi- and tri-modal distributions which suggests that there are differences among the penguin species. 

Lastly, it is noted that Adelie and Chinstrap measurements overlap for all variables except bill length. This feature is a definitive variable that produces complete separation among the penguin species into groups. This perfectly discriminating variable will be removed to get a reasonable estimate for the variables that can predict the outcome variable.

```{r pengsumm}
dfSummary(penguins, plain.ascii = TRUE, style = "grid", graph.col = FALSE, footnote = NA)
```

#### Data Preparation 

The summary above indicates the amount of missing data the penguin data contains. It appears that more than 3% of the missing data was from the `sex` variable. This further suggests that nearly 97% were complete. There were no missingness patterns, and their overall proportion was not very extreme. As a result, missingness can be corrected by imputation.

Further exploration revealed that no variable seems to be strongly influenced by any outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample. Outliers in the data could distort predictions and affect the accuracy, therefore, these would need to be corrected. 

To build a smaller model without predictors with extremely high correlations, it is best to reduce the number of predictors such that there were no absolute pairwise correlations above 0.90. The correlogram below graphically represents the correlations between the numeric predictor variables, when ignoring the missing variables. Most of the numeric variables were uncorrelated with one another, but there were a few highly correlated pairs. From the correlogram, the relationship between the `body_mass_g` and `flipper_length_mm` is a highly positive correlation, and within reason, as larger flippers would indicate an increase in body mass. There are some variables with moderate correlations, but their relationship is also intuitive. However, no relationship was too extreme, and it is clear that Adelie and Chinstrap overlap for all variable measurements except bill length. This feature is identified as the definitive variable that produces complete separation among the penguin species into groups.
 
```{r pengcorrgram, fig.height=5.5}
ggpairs(penguins, columns = 3:6, title = "Correlogram of Variables", 
        ggplot2::aes(color = species),
        progress = FALSE, 
        lower = list(continuous = wrap("smooth", alpha = 0.3, size = 0.1))) 
```

##### Training & Testing Split

The models were trained on the same approximately 70% of the data set, reserving 30% for validation of which model to select for the species class on the test set. This will allow for the test via cross-validation scheme of the model to tune parameters for optimal performance. 

```{r split}
# Create training and testing split
set.seed(525)
intrain = createDataPartition(penguins$species, p = 0.70, list = FALSE)

# Train & Test predictor variables
train_peng.p = penguins[intrain, -c(1,8)] # remove species, and year
test_peng.p = penguins[-intrain, -c(1,8)] 

# Train & Test response variable (species)
train_peng.r = penguins$species[intrain]
test_peng.r = penguins$species[-intrain]
```

##### Pre-Processing of Predictors

Missing data are treated by imputation. The classification and regression trees (CART) missing data algorithm was implemented because this could handle mixed types of missing data, and adaptable to interactions and non-linearity. 

```{r pengprepro}
set.seed(525)
temp = mice(train_peng.p, method = 'cart', print = FALSE, m = 3, maxit = 3)
train_peng.p = complete(temp)

temp = mice(test_peng.p, method = 'cart', print = FALSE, m = 3, maxit = 3)
test_peng.p = complete(temp)
```

##### Normality & Linearity 

The data were then pre-processed to fulfill the assumption of normality by centering and scaling.

```{r normality}
set.seed(525)
# Train set
processed_train_peng = preProcess(train_peng.p)
train_peng.p = predict(processed_train_peng, train_peng.p)

# Test set
processed_test_peng = preProcess(test_peng.p)
test_peng.p = predict(processed_test_peng, test_peng.p)
```

##### Dummy Variables

The categorical variables are then dummyfied. For instance, in the variable `sex`, the female will be used as the reference, whereas in the `island` variable, Biscoe island will be used as the reference.

```{r pengdummyVars}
set.seed(525)
# Train set
train_peng.pd = dummy.data.frame(train_peng.p, names = c("island","sex") , sep = ".")
train_peng.p = cbind(train_peng.p, train_peng.pd[,c(1:3,8:9)])
train_peng.p[sapply(train_peng.p, is.factor)] = data.matrix(train_peng.p[sapply(train_peng.p, is.factor)])
train_peng.p[,c(6:11)] = lapply(train_peng.p[,c(6:11)], factor) 
train_peng.p$island = factor(train_peng.p$island)
  
# Test set 
test_peng.pd = dummy.data.frame(test_peng.p, names = c("island","sex") , sep = ".")
test_peng.p = cbind(test_peng.p, test_peng.pd[,c(1:3,8:9)])
test_peng.p[sapply(test_peng.p, is.factor)] = data.matrix(test_peng.p[sapply(test_peng.p, is.factor)])
test_peng.p[,c(6:11)] = lapply(test_peng.p[,c(6:11)], factor) 
test_peng.p$island = factor(test_peng.p$island)
```

##### Feature Selection

To identify which features are important when building predictive model, feature selection is conducted to assist in choosing variables that are useful in predicting the response. The possible features that are impactful to classifying penguin species are listed below. This was done by using the random forest algorithm to performs a top-down search for relevant features and comparing the original attributes' importance with the importance achievable at random. It shows that `bill_length_mm` is indeed the most contributing variable followed by `flipper_length_mm`, and so on. 

```{r pengboruta}
output = Boruta(train_peng.r ~ ., data = train_peng.p, doTrace = 0)  
roughFixMod = TentativeRoughFix(output)
importance = attStats(TentativeRoughFix(output))
importance = importance[importance$decision != 'Rejected', c('meanImp', 'decision')]
kable(head(importance[order(-importance$meanImp), ])) %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

All in all, the following decision were made based on the feature selection investigation:

* The `flipper_length_mm` and `bill_depth_mm` are the most likely contributing variables that will be in the model.  
* The variable `island` is kept and evaluated per model on how much of a contribution difference it makes based on the algorithm and algorithm assumptions.  
* However, the `bill_length_mm` variable is removed due to it being a perfectly discriminating variable.   
* Due to high correlation with `flipper_length_mm`, `body_mass_g` is removed to avoid collinearity.  
* The `sex` variable is removed as it does not contribute based on the feature selection investigation.   
* The `year` variable is ignored.  

#### Building the Model

K-Nearest Neighbor classification works such that for each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random.

With the decision on the features set, there is no need to perform stepwise elimination to account for the best predictors, thus to optimize each model, 10 repeats of 10-fold cross-validation is perform. With accuracy being the decision metric for the best performing model, there is 10 repeats of the 10-fold cross-validation. By doing this, the training set is divided randomly into 10 parts and then each of 10 parts is used as testing set for the model trained on other the 9. Then the average of the 10 error terms is obtained by performing the 10-fold CV ten times. The advantage of a repeated hold-out instead of a k-fold is that there have more control.

```{r pengknn}
set.seed(525)
knnModel = train(x = train_peng.p[, c(3:4)], 
                 y = train_peng.r,
                 method = "knn",
                 trControl = trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10))
```

```{r pengmodel}
knnModel
```

```{r pengplot,fig.height=4, fig.width=8}
plot(knnModel, main = "Accuracy of KNN Model")
plot(varImp(knnModel), main = "Rank of Most Important Variable")
```

#### Model Discussion & Conclusions

The best tune for the KNN model which resulted in the largest accuracy is k = 5. It has accuracy = 80.5%, and $\kappa$ = 0.69. This tune accounts for the largest portion of the variability in the data than all other latent variables. Moreover, the variables that contributed the most to identifying Adelie and Gentoo is the flipper length, while bill depth was the most important variable to help classify Chinstrap. From the results based on the test data, the KNN model did exceptionally well in classifying the test set. Thus, the optimal model has an accuracy of 80.4% and $\kappa$ = 0.68 on the test set. 

In terms of the confusion matrix, the results suggest that 80.4% of the predicted results seem to be correctly classified. The precision for each type of species is also high (Adelie = 74%, Chinstrap = 54%, and Gentoo = 100%), suggesting that the penguins belong to the actual species among all the penguins predicted to be that particular species, with Gentoos being classified correctly 100% of the time. Moreover, the recall highlights that 87% of the Adelie species have been correctly classified accordingly, whereas 35% of the Chinstrap species have been correctly classified, and 97% of the Gentoo species have been correctly classified. In all, this model is capable of classifying penguins into one of the three species with great accuracy, particularly Gentoo species which was expected as their measurements were quite different. And lastly, the Kappa statistic of 0.68 suggests that the overall accuracy of this model is better than the expected random chance classifier's accuracy.

```{r pengCMx}
set.seed(525)
# Confusion Matrix
pred.R = predict(knnModel, newdata = test_peng.p, type = "raw")
confusion = confusionMatrix(pred.R, test_peng.r, mode = "everything")
confusion
```

Next, a receiver operating characteristic (ROC) analysis is shown in Figure 1. The area under the curve (AUC) for each class was estimated for observed penguin species and their predicted values by fitting the KNN model. The multi-class area under the curve for the predicted penguin species is the mean for all three AUC. It was computed to be 0.854. That is, there is a 85.4% chance that the model will be able to distinguish among the three penguin species. 

<center> Fig 1: ROC Curves of the KNN Model </center>

```{r pengROC}
predictions = as.numeric(predict(knnModel, test_peng.p, type = 'raw'))
roc.multi = multiclass.roc(test_peng.r, predictions)
auc(roc.multi)
plot.roc(roc.multi[['rocs']][[1]], main = "Multi-class ROC, Macro-Average ROC = 0.854")
sapply(2:length(roc.multi[['rocs']]), function(i) lines.roc(roc.multi[['rocs']][[i]], col=i))

legend("bottomright", 
       legend = c("ROC curve of Chinstrap",
                  "ROC curve of Gentoo",
                  "ROC curve of Adelie"), 
       col = c("black", "red", "green"), lwd = 2)

```

Given the `palmerpenguins` dataset, five multi-classification models, namely multinomial logistic regression, linear discriminant analysis, quadratic discriminant analysis, Naive Bayes and K-nearest neighbor models were fitted. From previous analysis, each model had its strengths and weakness, and in the end, the results were as follows:

Model | Accuracy
------|------
Multinomial Logistic Regression | 0.87
Linear Discriminant Analysis | 0.82
Quadratic Discriminant Analysis | 0.83
Naive Bayes | 0.92
K Nearest Neighbor | 0.80

Of all the classification model used to classify the penguin species, the Naive Bayes model's ability was proved to be near-optimal. Adelie and Gentoo were seen to be classified easily based on the flipper length, as it was the most important variable used in the classification. Whereas, for Chinshtrap, it was the bill depth. In conclusion, the Naive Bayes classifier produced a model that is 92.1% accurate in correctly classifying penguins into `Adelie`, `Chinstrap`, and `Gentoo`. This model also had an error rate of 0.161 between the measurements, which is the smallest than what the other models determined.

***

### Loan Approval 

#### Data Exploration 
The **loan approval** dataset will be used for the remaining models. 

*Loan Approvals Data Column Definition*

Variable | Description
----|------
Loan_ID | Unique Loan ID
Gender | Male/Female
Married | Applicant married (Y/N)
Dependents | Number of dependents
Education | Applicant Education (Graduate/Undergraduate)
Self_Employed | Self employed (Y/N)
ApplicantIncome | Applicant Income
CoapplicantIncome | Coapplicant Income
LoanAmount | Loan amount in thousands
Loan_Amount_Term | Term of loan in months
Credit_History | Credit history meets guidelines
Property_Area | Urban/Semi Urban/Rural
Loan_Status | (Target) Loan Approved (Y/N)

```{r loan_data}
loan_df_link <- 'https://raw.githubusercontent.com/greeneyefirefly/DATA622-Group3/main/Project_3/Loan_approval.csv'
loan_df <- read.csv(loan_df_link) %>%
  mutate_all(na_if,"") %>%
  mutate(Gender = as.factor(Gender),
         Married = as.factor(Married), 
         Dependents = as.factor(Dependents),
         Education = as.factor(Education),
         Self_Employed = as.factor(Self_Employed),
         Credit_History = as.factor(Credit_History),
         Property_Area = as.factor(Property_Area),
         Loan_Status = as.factor(Loan_Status)) %>%
  select(-Loan_ID)
```

This dataset includes `r nrow(loan_df)` datapoints and `r ncol(loan_df) +1` columns. The target variable is `Loan_Status`. Since the `Loan_ID` variable is unique to each record, we will remove it from the dataset. We can see that the `Loan_Status` classification is highly imbalanced, with more than double the amount of approvals (Y) than rejections (N). 

```{r loan_dist, fig.height=3}

loan_df %>%
  drop_na() %>%
  count(Loan_Status) %>%
  ggplot() + geom_col(aes(x = Loan_Status, y = n, fill = Loan_Status)) +
  geom_label(aes(x = Loan_Status, y = n, label = n)) +
  theme_minimal() +
  labs(title = 'Distribution of Loan Status')

```

#### Understanding the Data
Let's take a preliminary look at the summary statistics for the dataset: 

```{r loan_summary}
dfSummary(loan_df, plain.ascii = TRUE, style = "grid", graph.col = FALSE, footnote = NA)
```

Some things to note: 

* Seven of the variables have missing values, which is something we will have to deal with later on. 
* Almost all of the categorical variables are highly imbalanced: `Gender` (more males than females), `Married` (more married loan applicants than single), `Education` (more graduates than non-graduates), `Self_Employed` (less self-employed individuals), and `Credit History` (more individuals with credit history than not).

##### Categorical Features

We'll look at each of the categorical features with respect to the final classification. Since most of our categorical features are imbalanced, we will look at the data in terms of percentages as opposed to counts. 

`Gender`: Regardless of the sex, around 70% of individuals are approved for a loan. 

```{r loan_gender, fig.height=3}
tab_gender <- with(loan_df, table(Gender, Loan_Status))
tab_gender <- as.data.frame(prop.table(tab_gender, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_gender %>% 
  ggplot() +  
  geom_col(aes(x=Gender, y=Freq, fill=Gender)) +
  geom_label(aes(x=Gender, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Gender')

```

`Married`: Married individuals tend to be approved more often than non-married individuals. 

```{r loan_marriage, fig.height=3}
tab_married <- with(loan_df, table(Married, Loan_Status))
tab_married <- as.data.frame(prop.table(tab_married, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_married %>% 
  ggplot() +  
  geom_col(aes(x=Married, y=Freq, fill=Married)) +
  geom_label(aes(x=Married, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Marital Status')

```

`Dependents`: The number of dependents an individual has doesn't appear to be as indicative of loan approval. 

```{r loan_kids, fig.height=3}
tab_kids <- with(loan_df, table(Dependents, Loan_Status))
tab_kids <- as.data.frame(prop.table(tab_kids, margin = 1)) %>%
  filter(Loan_Status == 'Y')


tab_kids %>% 
  ggplot() +  
  geom_col(aes(x=Dependents, y=Freq, fill=Dependents)) +
  geom_label(aes(x=Dependents, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Number of Dependents')
```

`Education`: Graduates tend to be approved more often than non-graduates. 

```{r loan_edu, fig.height=3}
tab_edu <- with(loan_df, table(Education, Loan_Status))
tab_edu <- as.data.frame(prop.table(tab_edu, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_edu %>% 
  ggplot() +  
  geom_col(aes(x=Education, y=Freq, fill=Education)) +
  geom_label(aes(x=Education, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Education')
```

`Self-Employed`: Employment Status alone doesn't appear to have a significant impact on approval status.

```{r loan_emp, fig.height=3}
tab_emp <- with(loan_df, table(Self_Employed, Loan_Status))
tab_emp <- as.data.frame(prop.table(tab_emp, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_emp %>% 
  ggplot() +  
  geom_col(aes(x=Self_Employed, y=Freq, fill=Self_Employed)) +
  geom_label(aes(x=Self_Employed, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Employment Status')
```

`Credit History`: This factor has a really large impact on the final approval! 80% of individuals with credit history are approved versus only 8% for those with no credit history. 

```{r loan_credit, fig.height=3}
tab_cred <- with(loan_df, table(Credit_History, Loan_Status))
tab_cred <- as.data.frame(prop.table(tab_cred, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_cred %>% 
  ggplot() +  
  geom_col(aes(x=Credit_History, y=Freq, fill=Credit_History)) +
  geom_label(aes(x=Credit_History, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Credit History')
```

`Property_Area`: Individuals living in semi-urban areas tend to be approved more often than those in rural or urban areas.  

```{r loan_prop, fig.height=3}
tab_prop <- with(loan_df, table(Property_Area, Loan_Status))
tab_prop <- as.data.frame(prop.table(tab_prop, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_prop %>% 
  ggplot() +  
  geom_col(aes(x=Property_Area, y=Freq, fill=Property_Area)) +
  geom_label(aes(x=Property_Area, y=Freq, label = round(Freq,2))) +
  theme_minimal() +
  labs(title = 'Approved Loans by Property Area')
```

##### Numeric Features 

We can also take a look at the numeric features. 

`Applicant Income`: We see a few outliers in both approvals and non-approvals. The distribution for each classification is right-skewed. 

```{r loan_income, fig.height=3}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=ApplicantIncome, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Applicant Income')
```

`Coapplicant Income`: Once again, a few very high outliers. Data is right-skewed for both classes. 
```{r loan_coincome, fig.height=3}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=CoapplicantIncome, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Coapplicant Income')
```

`Loan Amount`: A few outliers in each class, but a normal distribution otherwise. 
```{r loan_amount, fig.height=3}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=LoanAmount, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Loan Amount')
```

`Loan Term`: Most applicants have a loan term between 300 - 400 months. 
```{r loan_term, fig.height=3}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=Loan_Amount_Term, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Loan Amount Term')
```

#### Data Preparation

We can see that 5 categorical and 2 numeric variables have missing values. 

```{r loan_missing}

loan_missing <- loan_df %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num_null = n()) %>%
  mutate(pct_missing = round(num_null / total * 100,2)) %>%
  filter(isna == TRUE) 

loan_missing %>%
  ggplot() + 
  geom_col(aes(x = key, y = pct_missing), fill='steelblue') +
  geom_label(aes(x = key, y = pct_missing, label = pct_missing)) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(title = 'Percent Missing by Variable', x = 'Variable', y = 'Percent Missing')

```

* We will discard all of the records that have missing categorical features. These will not be easy to impute and some (like Credit History) are impactful in the final classification.
* We will impute the values for the numeric features using the Multivariate imputation by chained equations (MICE) method. Multiple imputation involves creating multiple predictions for each missing value, helping to account for the uncertainty in the individual imputations. 

```{r loan_impute}

# remove records with null categorical values
loan_df <- loan_df %>% filter(!is.na(Married) & 
                                !is.na(Gender) & 
                                !is.na(Dependents) &
                                !is.na(Self_Employed) &
                                !is.na(Credit_History))

# impute numeric null values using MICE method
loan_df <- complete(mice(data = loan_df,
                         method = "pmm", print = FALSE), 3)

```


The final dataset contains `r nrow(loan_df)` records with `r ncol(loan_df)` columns. 


#### Training & Analysis

##### Train, Test, Split

```{r loan_tts}
set.seed(525)
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(loan_df), 
                      replace = TRUE, 
                      prob = c(0.7, 0.3))

train_set <- loan_df[which_train, ]
test_set <- loan_df[!which_train, ]
```

The three models will be trained on 70% of the dataset and validated on the remaining 30% of the set. The training set contains `r nrow(train_set)` records and the test set contains `r nrow(test_set)` records. 


##### Model #1: Decision Trees

##### Model #2: Random Forest

A classification ensemble method, the random forests algorithm seeks to address the challenge of tree correlation inherent to bagging decision trees. By linearly combining many individual, independent trees, it reduces variance in prediction. However, it also randomly selects predictor features at each split of each tree in the ensemble, which mitigates possible correlation. This combination makes it a strong performer in general.

Random forests modeling starts with feature selection to assess feature importance in predicting `Loan_Status`. This selection is itself done via an implementation of random forests. While the results of this process do not inform the subsequent random forest model and its features--all features are included in that model--they offer a high-level check of importance.   

```{r loanboruta}
set.seed(525)
output = Boruta(train_set$Loan_Status ~ ., data = train_set, doTrace = 0)  
roughFixMod = TentativeRoughFix(output)
importance = attStats(TentativeRoughFix(output))
importance = importance[importance$decision != 'Rejected', c('meanImp', 'decision')]
kable(head(importance[order(-importance$meanImp), ])) %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

Four features perform at least as well as the best performing randomized feature. With a mean importance of approximately 52.17, `Credit_History` represents the most important in predicting `Loan_Status`. The latter three--`CoapplicantIncome`,`ApplicantIncome`, and `Property_Area`--trail far behind.

EDA revealed that the numeric features, which all describe dollar amounts, skewed rightward. In attempts to address that skewness, the features undergo the Box-Cox transformation and subsequent normalization prior to modeling.

```{r loantransform}
train_set <- train_set %>% select(c("ApplicantIncome","CoapplicantIncome","LoanAmount","Loan_Amount_Term")) %>% preProcess(method = c("BoxCox", "center", "scale")) %>% predict(train_set)
test_set <- test_set %>% select(c("ApplicantIncome","CoapplicantIncome","LoanAmount","Loan_Amount_Term")) %>% preProcess(method = c("BoxCox", "center", "scale")) %>% predict(test_set)
```

As noted, the random forests model incorporates all possible features, including dummies associated with the levels of factors like `Gender`, `Education`, and `Property_Area`. The sole tuning parameter for this implementation in *caret* is $m_{try}$, which refers to the number of randomly selected features to be chosen at each tree split and has been shown to affect model accuracy. A vector of $m_{try}$ values from 2 through 10 is provided for grid search. The number of trees in the forest ensemble is another general parameter, though one considered less important by Kuhn and Johnson, who suggest 1000 trees as a reasonable number (2013). Lastly, a 10-fold cross-validation scheme is repeated three times to create the final model.

```{r rf}
set.seed(525)
(loan_rf <- train(Loan_Status ~ .,
                  data = train_set,
                  method = "ranger",
                  metric = "Accuracy",
                  num.trees = 1000,
                  importance = "impurity",
                  tuneGrid = expand.grid(.mtry=c(2:10), .min.node.size=1, .splitrule="gini"),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3, allowParallel = TRUE, classProbs = TRUE)))
```

Optimizing accuracy at approximately 0.791, the final model used three features at each split ($m_{try}$ = 4). The plot below depicts differences in accuracy across $m_{try}$ values.

```{r rfplot}
plot(loan_rf, main = "Optimal mtry value of 4 (Accuracy ~ 0.791)")
```

Considering feature importance, `Credit_History1`, representing rows with a credit history, is important in all trees, just as it was the most important in initial feature selection. `ApplicantIncome` (~ 66.08), `LoanAmount` (~ 61.36), `CoapplicantIncome` (~ 37.61), and `Loan_Amount_Term` (~ 15.49) are also relatively important, though none of them were notably so initially. In sum, given these data, an individual's having a credit history is clearly the most important predictor of loan receipt, whereas other predictors are a mixed bag.

```{r rfimportance}
varImp(loan_rf, scale = TRUE)
```

Predicting `Loan_Status` for the test set returns an accuracy of approximately 0.835 with $\kappa$ of approximately 0.56. These values both exceed their training set counterparts, which is surprising. It could simply result from sampling, though further investigation--beyond this exercise--seems warranted.

```{r rfpred}
loan_rf_pred <- predict(loan_rf, newdata = test_set)
postResample(pred = loan_rf_pred, obs = test_set$Loan_Status)
```

Unfortunately, `Loan_Status` is not well balanced across classes. Specifically, the data set contains roughly twice as many "Y" values as there are "N" values. This imbalance suggests that accuracy may not be the best measure of the random forest model's predictive performance. In response, a confusion matrix and other evaluative measures are run on the predictions. 

```{r rfmatrix}
(confusionMatrix(loan_rf_pred, test_set$Loan_Status, mode = "everything", positive = "Y"))
```

The confusion matrix shows poor performance in predicting loan application rejections ("N"), with a specificity, or true negative rate, of approximately 0.553. By comparison, the sensitivity, or true positive rate, is quite high, at approximately 0.955. Combining the two measures, the balanced accuracy of approximately 0.754 is middling. Below, an ROC curve carries an associated AUC value of approximately 0.754, which, again, is just okay.

```{r rfroc}
loan_rf_predictions <- cbind(test_set, loan_rf_pred = as.numeric(loan_rf_pred))
loan_rf_roc <- roc(loan_rf_predictions, Loan_Status, loan_rf_pred)
auc(loan_rf_roc)
plot.roc(loan_rf_roc, main = "AUC ~ 0.754")
```

Overall, the random forests model predicts approved loan applications well (`Loan_Status` = "Y") and rejected applications poorly (`Loan_Status` = "N"). It suffers from class imbalance in the available data, and this imbalance is felt in spite of cross-validation. Having additional observations for the negative class would be ideal, but otherwise, an extension of this extension should consider resampling the training set--perhaps over-sampling the negative class or testing additional resamples--to promote better balance across `Loan_Status` classes.

##### Model #3: Gradient Boosting


```{r}

# for reproducibility
set.seed(525)

# train GBM model
gbm.fit <- gbm(
  formula = Loan_Status ~.,
  distribution = "gaussian",
  data = train_set,
  n.trees = 10000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

# print results
print(gbm.fit)
```




```{r}
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))


# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")

```

We see that the CV error starts leveling off at 2648 trees. Here, we see that the minimum CV RMSE is 0.4055994 (this means on average our model is about 0.4055994 off from the actual loan status)

###### Tuning

```{r}
# for reproducibility
set.seed(525)

# train GBM model
gbm.fit2 <- gbm(
  formula = Loan_Status ~ .,
  distribution = "gaussian",
  data = train_set,
  n.trees = 5000,
  interaction.depth = 2,
  shrinkage = 0.01,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
```

We reduce the shrinkage from 0.001 to 0.01 and the number of trees ran to 5000. We also increase the depth of the trees from a single split to 2 splits. Compared to the above model, its RMSE is higher with 0.3931338.

```{r}
# find index for n trees with minimum CV error
min_MSE <- which.min(gbm.fit2$cv.error)

# get MSE and compute RMSE
sqrt(gbm.fit2$cv.error[min_MSE])


# plot loss function 
gbm.perf(gbm.fit2, method = "cv")

```


Manually tweaking each hyperparamters can be time consuming and less efficient than performing a grid seach which iterates over every combination of hyperparameter values and also allows to analyze which combination performs best.

First we want to construct our grid of hyperparameter conditions. We are doing a grid search across 81 models with different learning rates and tree depths.


```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)

```

Most of our top models use interaction.depth of 1 and a shrinkage of 0.30. The optimal trees are less than 15 for most of the top models. Comparing to the above models, the min_RMSE with the hyperparameter gave is lower by 0.06.

```{r}
# randomize data
random_index <- sample(1:nrow(train_set), nrow(train_set))
random_loan_train <- train_set[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(525)
  
  # train model
  gbm.tune <- gbm(
    formula = Loan_Status ~ .,
    distribution = "gaussian",
    data = train_set,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

So for the next hyperparameter grid, we will be tuning some of the parameters. First we will lower 5000 number of trees to 30, because the values for optimal trees are all less than 30. Most of the shrinkage is show as 0.3 and 0.1. Interaction.depth are 1, 3, 5. The n.minobsinnode values are 5, 10, 15. The bag.fraction values are 0.65, 0.80, and 1.00. We will modify the new grid with these new values.

```{r}
## Modifying hyperparameter grid

hyper_grid <- expand.grid(
  shrinkage = c(.01, .3, .1),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)


```

```{r}

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(525)
  
  # train model
  gbm.tune <- gbm(
    formula = Loan_Status ~ .,
    distribution = "gaussian",
    data = train_set,
    n.trees = 30,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)

```



###### Visualization

Using the new modifications we are able to obtain our best model:
shrinkage = 0.30, interaction.depth = 1, n.minobsinnode = 5, bag.fraction = 1.00, optimal_trees = 7. The min_RMSE for this model is 0.380. 


```{r}
# for reproducibility
set.seed(525)

# train GBM model
gbm.fit.final <- gbm(
  formula = Loan_Status ~ .,
  distribution = "gaussian",
  data = train_set,
  n.trees = 7,
  interaction.depth = 1,
  shrinkage = 0.3,
  n.minobsinnode = 5,
  bag.fraction = 1, 
  train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

```



```{r}

par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit.final, 
  cBars = 10,
  method = relative.influence, # also can use permutation.test.gbm
  las = 2
  )

```

We want to see which variables have the largest influence on Loan Status. We see Credit_History have the largest influence on Loan Status. 

cBars allows you to adjust the number of variables to show (in order of influence).

method = relative.influence: At each split in each tree, gbm computes the improvement in the split-criterion (MSE for regression). gbm then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important.

###### Predicting

```{r}
model_type.gbm <- function(x, ...) {
  return("regression")
}

predict_model.gbm <- function(x, newdata, ...) {
  pred <- predict(x, newdata, n.trees = x$n.trees)
  return(as.data.frame(pred))
}


```


```{r}
# predict values for test data
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, train_set)

train_set$Loan_Status<-as.numeric(train_set$Loan_Status)

# results
caret::RMSE(pred, train_set$Loan_Status)


```

Using the hyperparameter grid search as our final model, we want to use it to predict new observations. 




#### Model Performance 

#### Conclusions


### Works Cited

1. Horst AM, Hill AP, Gorman KB (2020). *palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0*. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218.
2. Kuhn M, Johnson K (2013). *Applied Predictive Modeling*. Springer Science+Business Media.
3. Wright MN, Wager S, Probst P (2020). *Package 'ranger'*. Accessed April 4, 2021 from https://cran.r-project.org/web/packages/ranger/ranger.pdf.

4.UC Business Analytics R Programming Guide. http://uc-r.github.io/gbm_regression

### Code Appendix

The code chunks below represent the R code called in order during the analysis. They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadpeng}
```
```{r pengdist, fig.width=8}
```
```{r pengsumm}
```
```{r pengcorrgram, fig.height=5.5}
```
```{r split}
```
```{r pengprepro}
```
```{r normality}
```
```{r pengdummyVars}
```
```{r pengboruta}
```
```{r pengknn}
```
```{r pengmodel}
```
```{r pengplot,fig.height=4, fig.width=8}
```
```{r pengCMx}
```
```{r pengROC}
```
```{r loan_data}
```
```{r loan_dist}
```
```{r loan_summary}
```
```{r loan_gender}
```
```{r loan_marriage}
```
```{r loan_kids}
```
```{r loan_edu}
```
```{r loan_emp}
```
```{r loan_credit}
```
```{r loan_prop}
```
```{r loan_income}
```
```{r loan_coincome}
```
```{r loan_amount}
```
```{r loan_term}
```
```{r loan_missing}
```
```{r loan_impute}
```
```{r loan_tts}
```
