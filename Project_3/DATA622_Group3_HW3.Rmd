---
title: "CUNY SPS DATA 622 - Machine Learning and Big Data"
subtitle: 'Spring 2021 - Group 3 - Homework 3'
author: "Maryluz Cruz, Samantha Deokinanan, Amber Ferger, Tony Mei, and Charlie Rosemond"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    theme: cerulean
    highlight: pygments
    toc: 3
urlcolor: purple
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center")
```

## R Packages

The `R` language is used to facilitate data modeling. The main `R` packages used for data wrangling, visualization, and graphics are listed below.

```{r libraries, echo=TRUE}
# Required R packages
library(palmerpenguins)
library(tidyverse)
library(kableExtra)
library(summarytools)
library(GGally)
library(caret)
library(mice)
library(dummies)
library(Boruta)
library(pROC)
library(rsample)
library(gbm)
library(party)
library(partykit)
library(rpart)
library(rpart.plot)
```

## Overview {.tabset .tabset-fade .tabset.-pills}

In this project, there are two analyses carried out using two different datasets: 1) the Palmer Penguins dataset and 2) a Loan Approvals dataset. Thus, this project is divided into two sections accordingly. 

Section 1, Palmer Penguins, investigates K-Nearest Neighbor (KNN) algorithm to predict species of penguin, and it is compared to previous five multi-classification models, namely Multinomial Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Naive Bayes classification models.

Section 2, Loan Approval, a dataset on loan approval status is used to predict loan approval using
Decision Trees, Random Forests, and Gradient Boosting Machines (GBM). Each analysis is conducted with a thorough explanation of the reasoning behind all the steps taken. In the end, there is a comparison of the model performance and the optimal model is selected.

The last two sections present references used during this analysis, and the written R codes in a technical appendx.

***
<center> **PROJECT SECTIONS** </center>
***

### Palmer Penguins 

#### Data Exploration 

The `palmerpenguins` data contains size measurements collected from 2007 - 2009 for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. For more information about this data collection, refer to  [palmerpenguins website.](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)

*Penguins Data Column Definition*

Variable | Description
----|------
species | penguin species (Ad√©lie, Chinstrap, and Gentoo)
island | island in Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)
bill_length_mm | bill length (millimeters)
bill_depth_mm | bill depth (millimeters)
flipper_length_mm | flipper length (millimeters)
body_mass_g | body mass (grams)
sex | penguin sex (female, male)
year | year data was collected

```{r loadpeng}
# Load dataset
penguins = penguins

# Number of observations
ntrobs = dim(penguins)[[1]]

# Converting Year to factor
penguins$year = as.factor(penguins$year)
```

The previous data exploration found that the response variable `species` denotes one of three penguin species, and a majority of the penguins are Adelie (n = 153), followed by Gentoo (n = 124) and Chinstrap (n = 68). The distribution between gender is nearly equally divided among the species but not for their island habitat. 

```{r pengdist, fig.width=8}
reorder <- function(x){
  factor(x, levels = names(sort(table(x), decreasing = TRUE)))
}

ggplot(drop_na(penguins), aes(x = reorder(species), fill = species)) + 
  geom_bar() +
  geom_text(stat = "count", aes(label =..count..), vjust=-0.5, size = 3) +
  facet_wrap(~sex) +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +
  theme(legend.position = "none")+
  labs(title = "Distibution of Species by Gender", y = "Frequency", x = "Species")
  
 ggplot(drop_na(penguins), aes(x = reorder(species), fill = species)) + 
  geom_bar() +
  geom_text(stat = "count", aes(label =..count..), vjust=-0.5, size = 3) +
  facet_wrap(~island) +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +
  theme(legend.position = "none")+
  labs(title = "Distibution of Species by Island Habitat", y = "Frequency", x = "Species")
```

There were `r ntrobs` observations of 4 numeric predictor variables and 2-factor predictor variables, namely `island`, and `sex`.  There is also a `year` variable that is ignored in this analysis. The data set did not have complete cases, and there is a presence of bi- and tri-modal distributions, which suggests that there are differences among the penguin species. 

Lastly, it is noted that Adelie and Chinstrap measurements overlap for all variables except bill length. This feature is a definitive variable that produces complete separation among the penguin species into groups. This perfectly discriminating variable will be removed to get a reasonable estimate for the variables that can predict the outcome variable.

```{r pengsumm}
dfSummary(penguins, plain.ascii = TRUE, style = "grid", graph.col = FALSE, footnote = NA)
```

#### Data Preparation 

The summary above indicates the amount of missing data in the penguin dataset. It appears that more than 3% of the missing data was from the `sex` variable. This further suggests that nearly 97% were complete. There were no missingness patterns, and their overall proportion was not very extreme. As a result, missingness can be corrected by imputation.

Further exploration revealed that no variable seems to be strongly influenced by any outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample. Outliers in the data could distort predictions and affect the accuracy, therefore, these would need to be corrected. 

To build a smaller model without predictors with extremely high correlations, it is best to reduce the number of predictors such that there were no absolute pairwise correlations above 0.90. The correlogram below graphically represents the correlations between the numeric predictor variables, when ignoring the missing variables. Most of the numeric variables were uncorrelated with one another, but there were a few highly correlated pairs. From the correlogram, the relationship between the `body_mass_g` and `flipper_length_mm` is a highly positive correlation, and within reason, as larger flippers would indicate an increase in body mass. There are some variables with moderate correlations, but their relationship is also intuitive. However, no relationship was too extreme, and it is clear that Adelie and Chinstrap overlap for all variable measurements except bill length. This feature is identified as the definitive variable that produces complete separation among the penguin species into groups.
 
```{r pengcorrgram, fig.height=5.5}
ggpairs(penguins, columns = 3:6, title = "Correlogram of Variables", 
        ggplot2::aes(color = species),
        progress = FALSE, 
        lower = list(continuous = wrap("smooth", alpha = 0.3, size = 0.1))) 
```

##### Training & Testing Split

The models were trained on the same approximately 70% of the data set, reserving 30% for validation of which model to select for the species class on the test set. This will allow for the test via cross-validation scheme of the model to tune parameters for optimal performance. 

```{r split}
# Create training and testing split
set.seed(525)
intrain = createDataPartition(penguins$species, p = 0.70, list = FALSE)

# Train & Test predictor variables
train_peng.p = penguins[intrain, -c(1,8)] # remove species, and year
test_peng.p = penguins[-intrain, -c(1,8)] 

# Train & Test response variable (species)
train_peng.r = penguins$species[intrain]
test_peng.r = penguins$species[-intrain]
```

##### Pre-Processing of Predictors

Missing data are treated by imputation using the classification and regression trees (CART) missing data algorithm. CART can handle mixed types of missing data and is adaptable to interactions and non-linearity. 

```{r pengprepro}
set.seed(525)
temp = mice(train_peng.p, method = 'cart', print = FALSE, m = 3, maxit = 3)
train_peng.p = complete(temp)

temp = mice(test_peng.p, method = 'cart', print = FALSE, m = 3, maxit = 3)
test_peng.p = complete(temp)
```

##### Dummy Variables

The categorical variables are then made sets of dummy variables. For instance, in the variable `sex`, the female will be used as the reference, whereas in the `island` variable, Biscoe island will be used as the reference.

```{r pengdummyVars}
set.seed(525)
# Train set
train_peng.pd = dummy.data.frame(train_peng.p, names = c("island","sex") , sep = ".")
train_peng.p = cbind(train_peng.p, train_peng.pd[,c(1:3,8:9)])
train_peng.p[sapply(train_peng.p, is.factor)] = data.matrix(train_peng.p[sapply(train_peng.p, is.factor)])
train_peng.p[,c(6:11)] = lapply(train_peng.p[,c(6:11)], factor) 
train_peng.p$island = factor(train_peng.p$island)
  
# Test set 
test_peng.pd = dummy.data.frame(test_peng.p, names = c("island","sex") , sep = ".")
test_peng.p = cbind(test_peng.p, test_peng.pd[,c(1:3,8:9)])
test_peng.p[sapply(test_peng.p, is.factor)] = data.matrix(test_peng.p[sapply(test_peng.p, is.factor)])
test_peng.p[,c(6:11)] = lapply(test_peng.p[,c(6:11)], factor) 
test_peng.p$island = factor(test_peng.p$island)
```

##### Feature Selection

To identify which features are important when building a predictive model, feature selection is conducted to assist in choosing variables that are useful in predicting the response. The possible features that have the most impact on classifying penguin species are listed below. This was done by using the random forest algorithm to performs a top-down search for relevant features and comparing the original attributes' importance with the importance achievable at random. It shows that `bill_length_mm` is indeed the most contributing variable followed by `flipper_length_mm`, and so on. 

```{r pengboruta}
output = Boruta(train_peng.r ~ ., data = train_peng.p, doTrace = 0)  
roughFixMod = TentativeRoughFix(output)
importance = attStats(TentativeRoughFix(output))
importance = importance[importance$decision != 'Rejected', c('meanImp', 'decision')]
kable(head(importance[order(-importance$meanImp), ]), 
      caption = "Feature Importance of Penguin Data") %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

All in all, the following decisions were made based on the feature selection investigation:

* The `flipper_length_mm` and `bill_depth_mm` are the most likely contributing variables that will be in the model.  
* However, the `bill_length_mm` variable is removed due to it being a perfectly discriminating variable.   
* Due to the high correlation with `flipper_length_mm`, `body_mass_g` is removed to avoid collinearity.  
* The `sex` variable is removed as it does not contribute based on the feature selection investigation.
* The `island` variable was kept and evaluated per model in the previous sections on how much of a contribution difference it makes based on the algorithm and algorithm assumptions. For the KNN model, it will be ignored. 
* The `year` variable is ignored.  

##### Normality & Linearity 

The data were then pre-processed to fulfill the assumption of normality, i.e., each quantitative variable $~N(\mu,\sigma) = (0,1)$ by centering and scaling. Normalization ensures that each variable has equal influence when calculating the Euclidean distance, else a variable with a larger scale would add bias to calculation.

```{r normality}
set.seed(525)
# Train set
processed_train_peng = preProcess(train_peng.p)
train_peng.p = predict(processed_train_peng, train_peng.p)

# Test set
processed_test_peng = preProcess(test_peng.p)
test_peng.p = predict(processed_test_peng, test_peng.p)
```

#### Building the KNN Model

KNN classification works such that for each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random.

With the decision on the features set, there is no need to perform stepwise elimination to account for the best predictors. To optimize each model, using accuracy as the criterion, 10 repeats of 10-fold cross-validation (CV) are performed. In this scheme, the training set is divided randomly into 10 parts and then each of 10 parts is used as a testing set for the model trained on other the 9. Then the average of the 10 error terms is obtained by performing the 10-fold CV ten times. A repeated hold-out offers greater control than a simple k-fold.

```{r pengknn}
set.seed(525)
knnModel = train(x = train_peng.p[, c(3:4)], 
                 y = train_peng.r,
                 method = "knn",
                 trControl = trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10))
```

```{r pengmodel}
knnModel
```

```{r pengplot,fig.height=4, fig.width=8}
plot(knnModel, main = "Accuracy of KNN Model")
plot(varImp(knnModel), main = "Rank of Most Important Variable")
```

#### Model Discussion 

Per accuracy, the best tune for the KNN model is k = 5. It has accuracy = 80.5%, and $\kappa$ = 0.69. This tune accounts for the largest portion of the variability in the data than all other latent variables. Moreover, the variable that contributed the most to identifying Adelie and Gentoo is the flipper length, while bill depth was the most important variable to help classify Chinstrap. From the results based on the test data, the KNN model did exceptionally well in classifying the test set. Thus, the optimal model has an accuracy of 80.4% and $\kappa$ = 0.68 on the test set. 

In terms of the confusion matrix, the results suggest that 80.4% of the predicted results seem to be correctly classified. The precision for each type of species is also high (Adelie = 74%, Chinstrap = 54%, and Gentoo = 100%), suggesting that the penguins belong to the actual species among all the penguins predicted to be that particular species, with Gentoos being classified correctly 100% of the time. Moreover, the recall highlights that 87% of the Adelie species have been correctly classified accordingly, whereas 35% of the Chinstrap species have been correctly classified, and 97% of the Gentoo species have been correctly classified. In all, this model is capable of classifying penguins into one of the three species with great accuracy, particularly Gentoo species which was expected as their measurements were quite different. And lastly, the Kappa statistic of 0.68 suggests that the overall accuracy of this model is better than the expected random chance classifier's accuracy.

```{r pengCMx}
set.seed(525)
# Confusion Matrix
pred.R = predict(knnModel, newdata = test_peng.p, type = "raw")
confusion = confusionMatrix(pred.R, test_peng.r, mode = "everything")
confusion
```

Next, a receiver operating characteristic (ROC) analysis is shown in Figure 1. The area under the curve (AUC) for each class was estimated for observed penguin species and their predicted values by fitting the KNN model. The multi-class area under the curve for the predicted penguin species is the mean for all three AUC. It was computed to be 0.854. That is, there is an 85.4% chance that the model will be able to distinguish among the three penguin species. 

<center> Fig 1: ROC Curves of the KNN Model </center>

```{r pengROC}
predictions = as.numeric(predict(knnModel, test_peng.p, type = 'raw'))
roc.multi = multiclass.roc(test_peng.r, predictions)
auc(roc.multi)
plot.roc(roc.multi[['rocs']][[1]], main = "Multi-class ROC, Macro-Average AUC = 0.854")
sapply(2:length(roc.multi[['rocs']]), function(i) lines.roc(roc.multi[['rocs']][[i]], col=i))

legend("bottomright", 
       legend = c("ROC curve of Chinstrap",
                  "ROC curve of Gentoo",
                  "ROC curve of Adelie"), 
       col = c("black", "red", "green"), lwd = 2)

```

#### Conclusion

Given the `palmerpenguins` dataset, five multi-classification models, namely multinomial logistic regression, LDA, QDA, Naive Bayes, and KNN models were fitted. From previous analyses, each model had its strengths and weakness on the same cleaned training dataset, and in the end, the results were as follows:

Model | Accuracy
------|------
Multinomial Logistic Regression | 0.87
Linear Discriminant Analysis | 0.82
Quadratic Discriminant Analysis | 0.83
*Naive Bayes* | *0.92*
K Nearest Neighbor | 0.80

Of all the classification models used to classify the penguin species, the Naive Bayes model's ability was proved to be near-optimal. Adelie and Gentoo were seen to be classified easily based on the flipper length, as it was the most important variable used in the classification. Whereas, for Chinshtrap, it was the bill depth. In conclusion, the Naive Bayes classifier produced a model that is 92.1% accurate in correctly classifying penguins into `Adelie`, `Chinstrap`, and `Gentoo`. This model also had an error rate of 0.161 between the measurements, which is the smallest than what the other models determined.

***

### Loan Approval 

#### Data Exploration 
The `loan approval` dataset will be used for the remaining models. 

*Loan Approvals Data Column Definition*

Variable | Description
----|------
Loan_ID | Unique Loan ID
Gender | Male/Female
Married | Applicant married (Y/N)
Dependents | Number of dependents
Education | Applicant Education (Graduate/Undergraduate)
Self_Employed | Self-employed (Y/N)
ApplicantIncome | Applicant Income
CoapplicantIncome | Co-applicant Income
LoanAmount | Loan amount in thousands
Loan_Amount_Term | Term of loan in months
Credit_History | Credit history meets guidelines
Property_Area | Urban/Semi-Urban/Rural
Loan_Status | (Target) Loan Approved (Y/N)

```{r loan_data}
loan_df_link <- 'https://raw.githubusercontent.com/greeneyefirefly/DATA622-Group3/main/Project_3/Loan_approval.csv'
loan_df <- read.csv(loan_df_link) %>%
  mutate_all(na_if,"") %>%
  mutate(Gender = as.factor(Gender),
         Married = as.factor(Married), 
         Dependents = as.factor(Dependents),
         Education = as.factor(Education),
         Self_Employed = as.factor(Self_Employed),
         Credit_History = as.factor(Credit_History),
         Property_Area = as.factor(Property_Area),
         Loan_Status = as.factor(Loan_Status)) %>%
  select(-Loan_ID)
```

This dataset includes `r nrow(loan_df)` data points and `r ncol(loan_df) +1` columns. The target variable is `Loan_Status`. Since the `Loan_ID` variable is unique to each record, it is removed from the dataset. It is clear that the `Loan_Status` classification is highly imbalanced, with more than twice the amount of approvals (*Y*) than rejections (*N*). 

```{r loan_dist, fig.height=3.5}
loan_df %>%
  drop_na() %>%
  count(Loan_Status) %>%
  ggplot() + geom_col(aes(x = Loan_Status, y = n, fill = Loan_Status), show.legend = FALSE) +
  geom_label(aes(x = Loan_Status, y = n, label = n)) +
  theme_minimal() +
  labs(title = 'Distribution of Loan Status', y = "Frequency", x = "Loan Status")
```

#### Understanding the Data

Below is highlights the summary statistics for the dataset: 

```{r loan_summary}
dfSummary(loan_df, plain.ascii = TRUE, style = "grid", graph.col = FALSE, footnote = NA)
```

It should be noted that: 

* Seven of the 12 variables have missing values, which will be corrected in the later sections. 
* The frequency of almost all of the categorical variables are unequally in proportion: `Gender` (more males than females), `Married` (more married loan applicants than single), `Education` (more graduates than non-graduates), `Self_Employed` (less self-employed individuals), and `Credit History` (more individuals with credit history than not).

##### Categorical Features

An in-depth investigation into each of the categorical features with respect to the final classification is conducted. Since most of the categorical features are disproportionate, the data is referred to in terms of percentages as opposed to counts. 

`Gender`: Regardless of the sex, around 70% of individuals are approved for a loan. 

```{r loan_gender, fig.height=3.5}
tab_gender <- with(loan_df, table(Gender, Loan_Status))
tab_gender <- as.data.frame(prop.table(tab_gender, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_gender %>% 
  ggplot() +  
  geom_col(aes(x=Gender, y=Freq, fill=Gender), show.legend = FALSE) +
  geom_label(aes(x=Gender, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Gender', y = "Percentage")
```

`Married`: Married individuals tend to be approved more often than non-married individuals. 

```{r loan_marriage, fig.height=3.5}
tab_married <- with(loan_df, table(Married, Loan_Status))
tab_married <- as.data.frame(prop.table(tab_married, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_married %>% 
  ggplot() +  
  geom_col(aes(x=Married, y=Freq, fill=Married), show.legend = FALSE) +
  geom_label(aes(x=Married, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Marital Status', y = "Percentage")
```

`Dependents`: The number of dependents an individual has does not appear to be indicative of loan approval. 

```{r loan_kids, fig.height=3.5}
tab_kids <- with(loan_df, table(Dependents, Loan_Status))
tab_kids <- as.data.frame(prop.table(tab_kids, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_kids %>% 
  ggplot() +  
  geom_col(aes(x=Dependents, y=Freq, fill=Dependents), show.legend = FALSE) +
  geom_label(aes(x=Dependents, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Number of Dependents', y = "Percentage", x = "# of Dependents")
```

`Education`: Graduates tend to be approved more often than non-graduates. 

```{r loan_edu, fig.height=3.5}
tab_edu <- with(loan_df, table(Education, Loan_Status))
tab_edu <- as.data.frame(prop.table(tab_edu, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_edu %>% 
  ggplot() +  
  geom_col(aes(x=Education, y=Freq, fill=Education), show.legend = FALSE) +
  geom_label(aes(x=Education, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Education', y = "Percentage")
```

`Self-Employed`: Employment Status alone does not appear to have a significant impact on approval status.

```{r loan_emp, fig.height=3.5}
tab_emp <- with(loan_df, table(Self_Employed, Loan_Status))
tab_emp <- as.data.frame(prop.table(tab_emp, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_emp %>% 
  ggplot() +  
  geom_col(aes(x=Self_Employed, y=Freq, fill=Self_Employed), show.legend = FALSE) +
  geom_label(aes(x=Self_Employed, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Employment Status', y = "Percentage", x = "Self-Employed")
```

`Credit History`: This factor appears to be more impactful on the final approval! Nearly 80% of individuals with a credit history are approved versus only 8% for those with no credit history. 

```{r loan_credit, fig.height=3.5}
tab_cred <- with(loan_df, table(Credit_History, Loan_Status))
tab_cred <- as.data.frame(prop.table(tab_cred, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_cred %>% 
  ggplot() +  
  geom_col(aes(x=Credit_History, y=Freq, fill=Credit_History), show.legend = FALSE) +
  geom_label(aes(x=Credit_History, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Credit History', y = "Percentage", x = "Credit History")
```

`Property Area`: Individuals living in semi-urban areas tend to be approved more often than those in rural or urban areas.  

```{r loan_prop, fig.height=3.5}
tab_prop <- with(loan_df, table(Property_Area, Loan_Status))
tab_prop <- as.data.frame(prop.table(tab_prop, margin = 1)) %>%
  filter(Loan_Status == 'Y')

tab_prop %>% 
  ggplot() +  
  geom_col(aes(x=Property_Area, y=Freq, fill=Property_Area), show.legend = FALSE) +
  geom_label(aes(x=Property_Area, y=Freq, label = paste0(round((Freq*100),1),"%"))) +
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme_minimal() +
  labs(title = 'Approved Loans by Property Area', y = "Percentage", x = "Property Area")
```

##### Numeric Features 

Next, an in-depth look into the numeric features is carried out to understand their influence on the loan approval status. 

`Applicant Income`: There are a few outliers in both approvals and non-approvals. The distribution for each classification is right-skewed. 

```{r loan_income, fig.height=3.5}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=ApplicantIncome, fill=Loan_Status)) +
  geom_histogram(alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Applicant Income', x = "Applicant Income")
```

`Co-applicant Income`: Once again, there are a few very high outliers. Data is right-skewed for both classes.

```{r loan_coincome, fig.height=3.5}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=CoapplicantIncome, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Coapplicant Income', x = "Co-applicant Income")
```

`Loan Amount`: A few outliers in each class, but a normal distribution otherwise. 

```{r loan_amount, fig.height=3.5}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=LoanAmount, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Loan Amount', x = "Loan Amount")
```

`Loan Term`: Most applicants have a loan term between 300 - 400 months. 

```{r loan_term, fig.height=3.5}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=Loan_Amount_Term, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Loan Amount Term', x = "Loan Amount Term")
```

#### Data Preparation

From the summary statistics, 5 categorical and 2 numeric variables have missing values. Their overall proportion is a bit extreme. For example, from the graph below, the first variable `Credit_history` accounts for 8% of the missing data when the other variables are complete, note this does not indicate within variable missingness. 

```{r loan_missing}
loan_missing <- loan_df %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num_null = n()) %>%
  mutate(pct_missing = round(num_null / total * 100,1)) %>%
  filter(isna == TRUE) 

loan_missing %>%
  ggplot() + 
  geom_col(aes(x = key, y = pct_missing), fill='steelblue') +
  geom_label(aes(x = key, y = pct_missing, label = paste0(pct_missing,"%"))) +
    scale_y_continuous(labels = function(x) paste0(x, "%")) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  labs(title = 'Percent Missing by Variable', x = 'Variable', y = 'Percent Missing')
```

As a result, 

* All of the records that have missing categorical features are discarded. These are not as easy to impute and some variables (like `Credit_History`) are impactful in the final classification.  
* Imputation for the numeric features is done using the Multivariate Imputation by Chained Equations (MICE) method. Multiple imputation involves creating multiple predictions for each missing value, helping to account for the uncertainty in the individual imputations. 

```{r loan_impute}
# remove records with null categorical values
loan_df <- loan_df %>% filter(!is.na(Married) & 
                                !is.na(Gender) & 
                                !is.na(Dependents) &
                                !is.na(Self_Employed) &
                                !is.na(Credit_History))

# impute numeric null values using MICE method
loan_df <- complete(mice(data = loan_df,
                         method = "pmm", print = FALSE), 3)
```

The final cleaned dataset contains `r nrow(loan_df)` records with `r ncol(loan_df)` columns. 

#### Training and Testing Split

All the models will be trained on the same approximately 70% of the training set, reserving 30% for validation of which model to select for the loan approval classification on the test set.

```{r loan_tts}
set.seed(525)
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(loan_df), 
                      replace = TRUE, 
                      prob = c(0.7, 0.3))

train_set <- loan_df[which_train, ]
test_set <- loan_df[!which_train, ]
```

The training set contains `r nrow(train_set)` records and the test set contains `r nrow(test_set)` records.

#### Feature Selection

The features of the training data set is investigated by calculating the mean importance. This selection is itself done via an implementation of random forests. While the results of this process do not inform the subsequent Model #2: Random Forest and its features, they offer a high-level check of importance and can be useful for the other models. 

```{r loanboruta}
set.seed(525)
output = Boruta(train_set$Loan_Status ~ ., data = train_set, doTrace = 0)  
roughFixMod = TentativeRoughFix(output)
importance = attStats(TentativeRoughFix(output))
importance = importance[importance$decision != 'Rejected', c('meanImp', 'decision')]
kable(head(importance[order(-importance$meanImp), ]), 
      caption = "Feature Importance of Loan Data Variables") %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

Four features perform at least as well as the best performing randomized feature. By looking at the table, the most contributing feature in the data is `Credit-History` with a mean importance that is above 50. `Credit_History` has already been suggestive of its importance from the variable exploration, and it is the feature that allows loan givers judge if individuals can pay for their bills on time, or if they are generally late with their payments. The other importance features that follow are `ApplicantIncome` and `CoapplicantIncome`, which is understandable as when it comes to loans, loan givers would want to reassurance that the individuals have enough income to repay for the loan.  

#### Normality & Linearity

The EDA revealed that the numeric features, which all describe dollar amounts, are skewed rightward. In attempts to address that skewness, these features undergo the Box-Cox transformation and subsequent normalization prior to modeling.

```{r loantransform}
set.seed(525)
train_set <- train_set %>% 
  select(c("ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term")) %>% 
  preProcess(method = c("BoxCox", "center", "scale")) %>% 
  predict(train_set)
test_set <- test_set %>% 
  select(c("ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term")) %>% 
  preProcess(method = c("BoxCox", "center", "scale")) %>% 
  predict(test_set)
```

#### Building the Models
##### Model #1: Decision Trees

Decision tree is a machine learning algorithm that is very versatile that is able to perform classification and regression tasks. When it comes to a decision tree, there are important terms that one should know, and these are: 
* root nodes - they represent the whole population or sample, which then gets divided within two or more.  
* splitting - is when the nodes get divided into two or more.  
* decision node - when a sub node splits.  
* terminal or leaf - when the node no longer splits.  
* branch - sub-section of the entire tree.  
* parent-node - is when a node is divided into sub-nodes.  

The decision tree is fitted on the training set, and visualized below.  

```{r loan_dt1, fig.height=10, fig.width=10}
set.seed(525)
dt_loan_model1 <- rpart(Loan_Status ~., 
                        data = train_set, 
                        control=rpart.control(cp = 0), 
                        method = 'class')
rpart.plot(dt_loan_model1)
```

The best first split is the one that provides the most information gain, and it shows that credit history plays an important role in the loan approval decision. 

In addition, this plot below shows the error rate of the model and it is of the complexity parameter (cp). Looking at the error, the best complexity parameter is at 0.01, because the x-error is at its lowest at 0.61. Moreover, in terms of the confusion matrix, (at the time that this was run) the decision tree model resulted in an accuracy rate of 80.4% and $\kappa$ at 48% on the test set. Looking at the predictions, there are 13 false positives, and 22 false negatives, while 98 are predicted correctly as an approved loan and 25 as no loan approval.

```{r loan_dt_plot, fig.height=4}
plotcp(dt_loan_model1)
print(dt_loan_model1$cptable)
```

```{r loan_dt_CMx, echo = FALSE}
unseen_prediction <-predict(dt_loan_model1, test_set, type = 'class')
confusionMatrix(table(unseen_prediction , test_set$Loan_Status))
```

The complexity parameter allows one to choose the optimal tree size. Thus, using this information from the base decision tree model, there is an attempt to improve the model and also compare the updated confusion matrix. 

##### Improved Decision Tree (with Hyperparameters)

For this decision tree model, there is the inclusion of the complexity parameter of 0.011 as a hyperparameter, which was obtained from the first model. One might wonder how does using complexity parameter change the decision, and by looking at the updated plot, the model has drastically changed. 

```{r loan_dt2, fig.height=4}
set.seed(525)
dt_improve <- rpart(Loan_Status ~.,
                    method="class",
                    data=train_set,
                    control=rpart.control(cp = 0.01),
                    parms=list(split='information'))
rpart.plot(dt_improve)

# train improved decision tree model
dt.model = train(Loan_Status ~.,
                 data=train_set,
                 method = "rpart",
                 metric = "Accuracy",
                 control = rpart.control(cp = 0.01),
                 trControl = trainControl(method = "repeatedcv",
                                          number = 10, 
                                          repeats = 3, 
                                          allowParallel = TRUE,
                                          classProbs = TRUE))
```

This only showcases `Credit_History` as the most important feature, which is not surprising since the mean importance was so high that no other feature was even close.  

The error rate of this model is just a straight line, and the accuracy and kappa statistic have improved after using the complexity parameter, accuracy = 82.9% and $\kappa$ = 0.52. The false positive is only 2, and the false negative is at 22.(at the time this was run) The improved model did better at predicting the loan approval status than the base decision tree model.

```{r loan_dt_CMx2}
unseen_prediction1 <-predict(dt.model, test_set, type = 'raw')
confusionMatrix(table(unseen_prediction1, test_set$Loan_Status))
```

##### Conditional Parting Plot 

Lastly, the plot below show the regression relationship of the conditional inference trees estimate that is partitioned by binary recursive in a framework that is conditional inference. It shows that credit history is the most important feature. Here you can see that, and that n = 56 is the number without a credit history, and n = 297 is the number with a credit history. 

```{r loan_dt_ctree}
set.seed(525)
ctree_<-ctree(Loan_Status ~., data=train_set)
plot(ctree_)
```

##### Model #2: Random Forest

As a classification ensemble method, the random forests algorithm seeks to address the challenge of tree correlation inherent to bagging decision trees. By linearly combining many individual, independent trees, it reduces variance in prediction. However, it also randomly selects predictor features at each split of each tree in the ensemble, which mitigates possible correlation. This combination makes it a strong performer in general.

As noted, random forests modeling starts with feature selection to assess feature importance in predicting `Loan_Status`. It incorporates all possible features, including dummies associated with the levels of factors like `Gender`, `Education`, and `Property_Area`. The sole tuning parameter for this implementation is $m_{try}$, which refers to the number of randomly selected features to be chosen at each tree split, and has been shown to affect model accuracy. A vector of $m_{try}$ values from 2 through 10 is provided for grid search. The number of trees in the forest ensemble is another general parameter, though one considered less important by Kuhn and Johnson (2013), who suggest 1000 trees as a reasonable number. Lastly, a 10-fold cross-validation scheme is repeated three times to create the final model.

```{r rf}
set.seed(525)
rf.model <- train(Loan_Status ~ .,
                  data = train_set,
                  method = "ranger",
                  metric = "Accuracy",
                  num.trees = 1000,
                  importance = "impurity",
                  tuneGrid = expand.grid(.mtry=c(2:10), .min.node.size=1, .splitrule="gini"),
                  trControl = trainControl(method = "repeatedcv", 
                                           number = 10, 
                                           repeats = 3, 
                                           allowParallel = TRUE, 
                                           classProbs = TRUE))
```

The resulting model optimized the accuracy to approximately 80%, and the final model used three features at each split ($m_{try}$ = 3). The plot depicts differences in accuracy across $m_{try}$ values. In addition, considering feature importance, `Credit_History` = 1, representing rows with a credit history, is important in all trees, just as it was the most important in the initial feature selection. `ApplicantIncome`, `LoanAmount`, `CoapplicantIncome`, and `Loan_Amount_Term` are also relatively important, though none of them were notably so initially. In summary, given these data, an individual's having a credit history is clearly the most important predictor of loan receipt, whereas other predictors are a mixed bag.

```{r rfplots, fig.width=10, fig.height=4}
p1 = plot(rf.model, main = "Optimal mtry = 3, Accuracy ~ 79.9%")
p2 = plot(varImp(rf.model, scale = TRUE), main = "Feature Importance: RF Model")
gridExtra::grid.arrange(p1,p2, ncol = 2)
```

Predicting `Loan_Status` for the test set returns an accuracy of approximately 84.2% with $\kappa$ of approximately 0.57. These values both exceed their training set counterparts, which is surprising. It could simply result from sampling, though further investigation -- beyond this exercise -- seems warranted.

```{r rfpred, echo = FALSE}
set.seed(525)
loan_rf_pred <- predict(rf.model, newdata = test_set)
stats = postResample(pred = loan_rf_pred, obs = test_set$Loan_Status)
```

Unfortunately, `Loan_Status` is not well balanced across classes. Specifically, the data set contains roughly twice as many "Y" values as there are "N" values. This imbalance suggests that accuracy may not be the best measure of the random forest model's predictive performance. In response, a confusion matrix and other evaluative measures are run on the predictions. 

```{r rfmatrix}
(confusionMatrix(loan_rf_pred, test_set$Loan_Status, mode = "everything", positive = "Y"))
```

The confusion matrix shows poor performance in predicting loan application rejections ("N"), with a specificity, or true negative rate, of approximately 0.51. By comparison, the sensitivity, or true positive rate, is quite high, at approximately 0.98. Combining the two measures, the balanced accuracy of approximately 0.75 is middling. Below, an ROC curve carries an associated AUC value of approximately 0.75, which, again, is just okay.

```{r rfroc}
loan_rf_predictions <- cbind(test_set, loan_rf_pred = as.numeric(loan_rf_pred))
loan_rf_roc <- roc(loan_rf_predictions, Loan_Status, loan_rf_pred)
auc(loan_rf_roc)
plot.roc(loan_rf_roc, main = "AUC ~ 0.75")
```

##### Model #3: Gradient Boosting

Gradient Boosted Machines (GBM) build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful ‚Äúcommittee‚Äù that are often hard to beat with other algorithms. The advantages of GBM are it provides predictive accuracy, lots of flexibility with hyperparameter tuning options, works well with categorical and numerical values and can handle missing data. The disadvantages of GBM can be overfitting due to constant error minimization and requires many trees which can be time consuming and memory intensive. Results might be less interpretable unless used with other tools like variable importance, partial dependence plots, `LIME`, etc. 

Boosting works by adding new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far. GBM starts with base-learning models, then training weak models and followed up with sequential training with respect to errors. The following is explained more in detail on the process.

* Base-learning models: Boosted algorithms almost always use decision trees as the base-learner (here, boosting is used in the context of regression trees).

* Training weak models: A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. Commonly, trees with only 1-6 splits are used. Combining many weak models increases speed, accuracy improvement, and avoids overfitting.

* Sequential training with respect to errors: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. 

The basic algorithm for boosted regression trees can be generalized to the following where x represents the predictor variables and y represents response variables:

1) Fit a decision tree to the data: $F_1(x) = y$,

2) We then fit the next decision tree to the residuals of the previous: $h_1(x) = y‚àíF_1(x)$,

3) Add this new tree to the algorithm: $F_2(x)=F_1(x)+h_1(x)$,

4) Fit the next decision tree to the residuals of $F_2: h_2(x)=y‚àíF_2(x))$,

5) Add this new tree to the algorithm: $F_3(x)=F_2(x)+h_1(x)$,

6) Continue this process until some mechanism (i.e. cross validation) tells it to stop.

To start, a base GBM is fitted with normal parameters, i.e. a small shrinkage, such between 0.01 and 0.001; the number of trees, `n.trees`, is between 3000 and 10000, and an interaction depth of 1.

```{r loan_gbm}
set.seed(525)

# Casting loan status as {0,1} for caret purposes
train_set_gbm <- train_set %>%  mutate(Loan_Status = ifelse(Loan_Status == "N",0,1))

# fit base GBM model
gbm.fit <- gbm(formula = Loan_Status ~.,
               distribution = "bernoulli",
               data = train_set_gbm,
               n.trees = 10000,
               interaction.depth = 1,
               shrinkage = 0.001,
               cv.folds = 5,
               n.cores = NULL, # will use all cores by default
               verbose = FALSE)  
```
```{r gbm_plots, fig.width=6, fig.height=4}
# plot best iteration number
p1 = gbm.perf(gbm.fit, method = "cv")
```

The plot above shows that the CV error starts leveling off at 3196 trees. So, the GBM is tuned to try to improve the model performance. The hyper-tuning parameters are:

* Number of trees: The total number of trees to fit. GBMs often require many trees.   
* Depth of trees: The number d of splits in each tree, which controls the complexity of the boosted ensemble. Each tree is a stump consisting of a single split.  
* Learning rate: Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called shrinkage.  
* Subsampling: Controls whether or not you use a fraction of the available training observations. This can help to minimize overfitting.   

```{r gbm_model2}
# for reproducibility
set.seed(525)

# train GBM model
gbm.fit2 <- gbm(formula = Loan_Status ~ .,
                distribution = "bernoulli",
                data = train_set_gbm,
                n.trees = 3000,
                interaction.depth = 2,
                shrinkage = 0.01,
                cv.folds = 5,
                n.cores = NULL, # will use all cores by default
                verbose = FALSE)  
```

##### Improved Gradient Boosting Model

Manually tweaking each hyperparamters can be time consuming, and less efficient than performing a grid search which iterates over every combination of hyperparameter values. This also allows to analyze which combination performs best. Therefore, a grid of hyperparameter conditions is constructed, and the algorithm will search across all the models with different learning rates and tree depths to determine the optimal fit. Firstly, there will be a lowering of the number of trees from 5000 to 30, because the values for optimal trees are all less than 30. Most of the shrinkage is shown as 0.3 and 0.1. `Interaction.depth` are 1, 3, 5. The `n.minobsinnode` values are 5, 10, 15. These will be used to create the new hyperparameter grid, and a new gradient boosting model is trained on the training set.

```{r loan_finalgbm}
# for reproducibility
set.seed(525)

# create hyperparameter grid
hyper_grid <- expand.grid(n.trees = c(1:30),
                          shrinkage = c(.01, .1, .3),
                          interaction.depth = c(1, 3, 5),
                          n.minobsinnode = c(5, 10, 15))
# create training control
control <- trainControl(method = "repeatedcv", 
                        number = 10, 
                        repeats = 3, 
                        allowParallel = TRUE, 
                        classProbs = TRUE)

# train final GBM model
gbm.model <- train(Loan_Status~.,
                   data = train_set,
                   method = "gbm", 
                   metric = "Accuracy",
                   trControl = control, 
                   tuneGrid = hyper_grid,
                   verbose = FALSE)
```

Accuracy is one minus the error rate and is thus the percentage of correctly classified observations. The best tuning parameters for the gradient boosting model which resulted in the largest accuracy, i.e. the overall predicted accuracy of the model, is with a n.trees = 30, interaction.depth = 3, shrinkage = 0.1, and n.minobsinnode = 15. It has a model accuracy of 79.6%, and $\kappa$ = 0.47. In this case, it does not account for the largest portion of the variability in the data than all other variables, but it produces the smallest error which makes it the optimal fitting parameters. 

In addition, considering feature importance, once again, as will the other models, `Credit_History` = 1, representing rows with a credit history, is most contributing variable in the model. Following it are `LoanAmount`, `ApplicantIncome`, `CoapplicantIncome`, and, interestingly, `Property_Area`. 

```{r rfplot, fig.width=5, fig.height=4}
stats = data.frame(Model = "GBM",
				   Accuracy = gbm.model$results$Accuracy[as.numeric(rownames(gbm.model$bestTune))],
				   Kappa = gbm.model$results$Kappa[as.numeric(rownames(gbm.model$bestTune))])

plot(varImp(gbm.model, scale = TRUE), main = "Feature Importance: GBM Model")
```

Predicting `Loan_Status` for the test set returns an accuracy of approximately 82.3% with $\kappa$ of approximately 0.51. These values both exceed their training set counterparts.

```{r gbmpred, echo = FALSE}
set.seed(525)
loan_gbm_pred <- predict(gbm.model, newdata = test_set)
stats = postResample(pred = loan_gbm_pred, obs = test_set$Loan_Status)
```

The confusion matrix results suggest that 82.3% of the predicted results seem to be correctly classified. Precision indicates how many of the correctly predicted cases actually turned out to be positive. With the gradient boosting model, the precision for loan approval was fairly good at 81.2%, whereas the recall, or how many of the actual positive cases that the model was able to predict correctly, highlights that 97.3% have been correctly classified. In all, this model is capable of classifying loan approval status with good accuracy. And lastly, the Kappa statistic of 0.51 suggests that the overall accuracy of this model is better than the expected random chance classifier's accuracy.

```{r gbmmatrix}
(confusionMatrix(loan_gbm_pred, test_set$Loan_Status, mode = "everything", positive = "Y"))
```

#### Model Performance 

In binary classification, often, the measures of focus are precision and recall, as opposed to accuracy. A measure combining this is the F1 score, defined as twice the sum of precision and recall divided by their product. Another oft-used metric is the area under the receiver operating curve, AUC. In this case, the following three metrics, i.e. Accuracy, F1, and AUC, will be compared to select the model that performs best in at least two of the three metrics.

Once again, below shows the confusion matrices for the three models, followed by a table with the selected performance criteria for comparison. 

```{r comparison}
# Create table to hold model comparison statistics
compTable = data.frame(Models = c('Model 1: Decision Tree', 
                                  'Model 2: Random Forest', 
                                  'Model 3: Gradient Boosting'),
                       ACC = double(3),
                       F1 = double(3),
                       AUC = double(3))
```

##### Model 1: Decision Tree
```{r m1TestSet}
# Model 1 Performance: Calculate accuracy statistics
pred.R = predict(dt.model, newdata = test_set, type = "raw")
pred.P = predict(dt.model, newdata = test_set, type = "prob")
m1CM = confusionMatrix(pred.R, test_set$Loan_Status, mode = "everything")
m1CM$table
compTable[1, 2] = sum(diag(m1CM$table)) / sum(m1CM$table)
compTable[1, 3] = m1CM$byClass[7]
compTable[1, 4] = auc(response = test_set$Loan_Status, predictor = pred.P[, 2])
```

##### Model 2: Random Forest
```{r m2TestSet}
# Model 2 Performance: Calculate accuracy statistics
pred.R = predict(rf.model, newdata = test_set, type = "raw")
pred.P = predict(rf.model, newdata = test_set, type = "prob")
m2CM = confusionMatrix(pred.R, test_set$Loan_Status, mode = "everything")
m2CM$table
compTable[2, 2] = sum(diag(m2CM$table)) / sum(m2CM$table)
compTable[2, 3] = m2CM$byClass[7]
compTable[2, 4] = auc(response = test_set$Loan_Status, predictor = pred.P[, 2])
```

##### Model 3: Gradient Boosting
```{r m3TestSet}
# Model 3 Performance: Calculate accuracy statistics
pred.R = predict(gbm.model, newdata = test_set, type = "raw")
pred.P = predict(gbm.model, newdata = test_set, type = "prob")
m3CM = confusionMatrix(pred.R, test_set$Loan_Status, mode = "everything")
m3CM$table
compTable[3, 2] = sum(diag(m3CM$table)) / sum(m3CM$table)
compTable[3, 3] = m3CM$byClass[7]
compTable[3, 4] = auc(response = test_set$Loan_Status, predictor = pred.P[, 2])
```

##### Selected Performance Statistics
```{r selperfstat}
# Display Model Comparison
kable(compTable, digits = 3L, caption = "Model Test Results") %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

#### Conclusion

All the models performed very well and classified the majority of the test set properly. The difference between the three models was very slight. False negatives were the lowest with the Random Forest model, than the other two, while there were more false positives with Gradient Boosting model. Given the above statistics and confusion matrices, **Model 2: Random Forest** barely edged out the other two models with the best accuracy and F-1 score, and is selected as the optimal model to predict loan approval.

This analysis highlights that loan approval data is strongly influenced by credit history and income of the individuals applying for the loan. Each  models presented it to have the most impact on the model. However, these models were not trained on a well-balanced dataset, and this imbalance is felt in spite of cross-validation. More specifically, even the optimal model, the random forest model, predicts approved loan applications well but rejected applications poorly. Having additional observations for the negative class would be ideal, but otherwise, it is recommended that any extension of this analysis should consider resampling the training set ‚Äì- perhaps over-sampling the negative class or testing additional resamples -‚Äì to promote better balance across `Loan_Status` classes. 

### Works Cited

1. Horst AM, Hill AP, Gorman KB (2020). *palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0*. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218.
2. Kuhn M, Johnson K (2013). *Applied Predictive Modeling*. Springer Science+Business Media.
3. Wright MN, Wager S, Probst P (2020). [*Package 'ranger'*.](https://cran.r-project.org/web/packages/ranger/ranger.pdf)
4. UC Business Analytics R Programming Guide.[*Gradient Boosting Machines.*](http://uc-r.github.io/gbm_regression)
5. DataCamp. [*What are the hyperparameters for a decision tree?.*](https://campus.datacamp.com/courses/tree-based-models-in-r/regression-trees?ex=7)
6. datascievo. [*Decision Tree in R | A Real Guide on Titanic Dataset with Code.*](https://www.datascievo.com/decision-tree-in-r/) 
7. Guru99. [*Decision Tree in R | Classification Tree & Code in R with Example.*](https://www.guru99.com/r-decision-trees.html)
8. DataFlair. [*R Decision Trees ‚Äì The Best Tutorial on Tree Based Modeling in R!*.](https://data-flair.training/blogs/r-decision-trees/)

### Code Appendix

The code chunks below represent the R code called in order during the analysis. They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadpeng}
```
```{r pengdist}
```
```{r pengsumm}
```
```{r pengcorrgram}
```
```{r split}
```
```{r pengprepro}
```
```{r normality}
```
```{r pengdummyVars}
```
```{r pengboruta}
```
```{r pengknn}
```
```{r pengmodel}
```
```{r pengplot}
```
```{r pengCMx}
```
```{r pengROC}
```
```{r loan_data}
```
```{r loan_dist}
```
```{r loan_summary}
```
```{r loan_gender}
```
```{r loan_marriage}
```
```{r loan_kids}
```
```{r loan_edu}
```
```{r loan_emp}
```
```{r loan_credit}
```
```{r loan_prop}
```
```{r loan_income}
```
```{r loan_coincome}
```
```{r loan_amount}
```
```{r loan_term}
```
```{r loan_missing}
```
```{r loan_impute}
```
```{r loan_tts}
```
```{r loanboruta}
```
```{r loantransform}
```
```{r loan_dt1}
```
```{r loan_dt_plot}
```
```{r loan_dt_CMx}
```
```{r loan_dt2}
```
```{r loan_dt_CMx2}
```
```{r loan_dt_ctree}
```
```{r rf}
```
```{r rfplots}
```
```{r rfpred}
```
```{r rfmatrix}
```
```{r rfroc}
```
```{r loan_gbm}
```
```{r gbm_plots}
```
```{r gbm_model2}
```
```{r loan_finalgbm}
```
```{r rfplot}
```
```{r gbmpred}
```
```{r gbmmatrix}
```
```{r comparison}
```
```{r m1TestSet}
```
```{r m2TestSet}
```
```{r m3TestSet}
```
```{r selperfstat}
```
